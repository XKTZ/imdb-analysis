{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "id": "oTkN8LU2BkPI",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1676578487348,
     "user_tz": 300,
     "elapsed": 2865,
     "user": {
      "displayName": "datas xktz",
      "userId": "10120887492997993635"
     }
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext.vocab\n",
    "from torch import nn\n",
    "import torchtext\n",
    "import  numpy as np\n",
    "import torchdata\n",
    "from torchtext.vocab import vocab\n",
    "from torchtext import transforms as T\n",
    "from typing import List, Tuple\n",
    "from transformer import Transformer, TransformerEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f1bd7c4e4d0>"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "SEED = 1234\n",
    "torch.manual_seed(SEED)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JWvKGdjtBkPM",
    "outputId": "8b62f3dc-2a89-4a39-9e0e-cf5e817a1500",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1676578487349,
     "user_tz": 300,
     "elapsed": 5,
     "user": {
      "displayName": "datas xktz",
      "userId": "10120887492997993635"
     }
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "tokenizer = get_tokenizer('basic_english')"
   ],
   "metadata": {
    "id": "QN6Ptn-wBkPN",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1676578487349,
     "user_tz": 300,
     "elapsed": 3,
     "user": {
      "displayName": "datas xktz",
      "userId": "10120887492997993635"
     }
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "train_iter, test_iter = torchtext.datasets.IMDB(root='./data')\n",
    "train_list = list(train_iter)\n",
    "test_list = list(test_iter)"
   ],
   "metadata": {
    "id": "v72_wN6-BkPO",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1676578487879,
     "user_tz": 300,
     "elapsed": 533,
     "user": {
      "displayName": "datas xktz",
      "userId": "10120887492997993635"
     }
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "MOST_COMMON_SIZE = 30000\n",
    "SENTENCE_LENGTH = 300\n",
    "EMBED_SIZE = 50\n",
    "MODEL_LOADING = None"
   ],
   "metadata": {
    "id": "-DFz8T7ABkPP",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1676578487880,
     "user_tz": 300,
     "elapsed": 2,
     "user": {
      "displayName": "datas xktz",
      "userId": "10120887492997993635"
     }
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from collections import Counter, OrderedDict\n",
    "\n",
    "counter = Counter()\n",
    "for (label, line) in train_list:\n",
    "    counter.update(tokenizer(line))"
   ],
   "metadata": {
    "id": "Xohoo-hCBkPP",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1676578491791,
     "user_tz": 300,
     "elapsed": 3913,
     "user": {
      "displayName": "datas xktz",
      "userId": "10120887492997993635"
     }
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "most_common_words = counter.most_common(MOST_COMMON_SIZE + 10)[10:]\n",
    "PAD = '<PAD>'\n",
    "UNK = '<UNK>'\n",
    "BOS = '<BOS>'\n",
    "EOS = '<EOS>'\n",
    "eng_vocab = vocab(OrderedDict(most_common_words), specials=[PAD, UNK, BOS, EOS])\n",
    "eng_vocab.set_default_index(eng_vocab[UNK])"
   ],
   "metadata": {
    "id": "JxKl-5VVBkPQ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1676578491791,
     "user_tz": 300,
     "elapsed": 3,
     "user": {
      "displayName": "datas xktz",
      "userId": "10120887492997993635"
     }
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "glove = torchtext.vocab.GloVe(name='6B', dim=EMBED_SIZE)"
   ],
   "metadata": {
    "id": "mmWufEsTsgVR",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1676578492311,
     "user_tz": 300,
     "elapsed": 522,
     "user": {
      "displayName": "datas xktz",
      "userId": "10120887492997993635"
     }
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "embedding_weight_matrix = [[0 for i in range(EMBED_SIZE)] for j in range(4)]\n",
    "embedding_weight_matrix.extend(\n",
    "    [glove.get_vecs_by_tokens(word[0]) for word in most_common_words]\n",
    ")\n",
    "embedding_weight_matrix = torch.tensor(embedding_weight_matrix)"
   ],
   "metadata": {
    "id": "6HVAAVTjBkPR",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1676578496226,
     "user_tz": 300,
     "elapsed": 3916,
     "user": {
      "displayName": "datas xktz",
      "userId": "10120887492997993635"
     }
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "VOCAB_SIZE = len(eng_vocab)\n",
    "\n",
    "class IMDBDataset(torch.utils.data.Dataset):\n",
    "    size: int\n",
    "    ret: list\n",
    "    targs: list\n",
    "    device: torch.device\n",
    "\n",
    "    def __init__(self, lst: list, dev: torch.device = None):\n",
    "        super().__init__()\n",
    "        self.ret = []\n",
    "        on = 0\n",
    "        self.size = len(lst)\n",
    "        self.device = torch.device('cpu:0') if dev is None else dev\n",
    "        self.targs = []\n",
    "        print('Loading Data Set')\n",
    "        tot = len(lst)\n",
    "        cnt = 0\n",
    "        for lbl, comment in lst:\n",
    "            dt = tokenizer(comment)\n",
    "            self.ret.append(\n",
    "                [eng_vocab[BOS]] + [eng_vocab[word] for word in dt[:SENTENCE_LENGTH]] + [eng_vocab[EOS]]\n",
    "            )\n",
    "            if len(dt) < SENTENCE_LENGTH:\n",
    "                self.ret[-1].extend([eng_vocab[PAD]] * (SENTENCE_LENGTH - len(dt)))\n",
    "            self.ret[-1] = torch.tensor(self.ret[-1], dtype=torch.long).to(device=dev)\n",
    "            self.targs.append(\n",
    "                torch.tensor([1, 0] if lbl == 1 else [0, 1],\n",
    "                             dtype=torch.float)\n",
    "                .to(device=dev)\n",
    "            )\n",
    "            cnt += 1\n",
    "            print(f\"\\r{cnt} / {tot}         \", end='')\n",
    "        print()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        return self.ret[idx], self.targs[idx]"
   ],
   "metadata": {
    "id": "Ff44xROJBkPR",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1676578496226,
     "user_tz": 300,
     "elapsed": 10,
     "user": {
      "displayName": "datas xktz",
      "userId": "10120887492997993635"
     }
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\")"
   ],
   "metadata": {
    "id": "Py5zPherxJ8M",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1676578496227,
     "user_tz": 300,
     "elapsed": 10,
     "user": {
      "displayName": "datas xktz",
      "userId": "10120887492997993635"
     }
    }
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading Data Set\n",
      "25000 / 25000         \n",
      "Loading Data Set\n",
      "25000 / 25000         \n"
     ]
    }
   ],
   "source": [
    "imdb_train_set = IMDBDataset(train_list)\n",
    "imdb_test_set = IMDBDataset(test_list)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J4bv91pGBkPT",
    "outputId": "faf205d9-c216-4b32-9a6f-abbed1c223e8",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1676578540973,
     "user_tz": 300,
     "elapsed": 44756,
     "user": {
      "displayName": "datas xktz",
      "userId": "10120887492997993635"
     }
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "train_data_loader = DataLoader(imdb_train_set, batch_size=128, shuffle=True)\n",
    "test_data_loader = DataLoader(imdb_test_set, batch_size=128, shuffle=True)"
   ],
   "metadata": {
    "id": "7OUyMYBUX6qf",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1676578540974,
     "user_tz": 300,
     "elapsed": 14,
     "user": {
      "displayName": "datas xktz",
      "userId": "10120887492997993635"
     }
    }
   },
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "\n",
    "    emb: nn.Module\n",
    "    transformer: TransformerEncoder\n",
    "    recurrent_part: nn.Module\n",
    "    predict: nn.Module\n",
    "\n",
    "    empty: int\n",
    "\n",
    "    def __init__(self, out_dim: int,\n",
    "                 transformer_layer: int = 6,\n",
    "                 gru_layer: int = 1, gru_dim: int = 512,\n",
    "                 forward_dim: int = 1024,\n",
    "                 empty: int = 0):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding.from_pretrained(embedding_weight_matrix, freeze=True)\n",
    "        self.transformer = TransformerEncoder(\n",
    "            transformer_layer, EMBED_SIZE\n",
    "        )\n",
    "        self.recurrent_part = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.GRU(EMBED_SIZE, gru_dim, num_layers=gru_layer, batch_first=True)\n",
    "        )\n",
    "        self.predict = nn.Sequential(\n",
    "            nn.Linear(gru_dim, forward_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(forward_dim, out_dim),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        self.empty = empty\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        msk = Transformer.get_attention_pad_mask(x, x.shape[1], self.empty) == 1\n",
    "        x = self.emb(x)\n",
    "        x = self.transformer(x, msk)\n",
    "        all_result, final_result = self.recurrent_part(x)\n",
    "        final_result = final_result[-1]\n",
    "        return self.predict(final_result)"
   ],
   "metadata": {
    "id": "GS_qMzztBkPS",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1676578540974,
     "user_tz": 300,
     "elapsed": 13,
     "user": {
      "displayName": "datas xktz",
      "userId": "10120887492997993635"
     }
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "model = Classifier(out_dim=2, empty=eng_vocab[PAD]).to(device)\n",
    "if MODEL_LOADING is not None:\n",
    "    model.load_state_dict(torch.load(MODEL_LOADING))"
   ],
   "metadata": {
    "id": "q8c7UkmGBkPT",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1676578544487,
     "user_tz": 300,
     "elapsed": 3526,
     "user": {
      "displayName": "datas xktz",
      "userId": "10120887492997993635"
     }
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "loss = nn.BCELoss().to(device)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=1e-5)"
   ],
   "metadata": {
    "id": "hli2iRYo-uAR",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1676578544488,
     "user_tz": 300,
     "elapsed": 4,
     "user": {
      "displayName": "datas xktz",
      "userId": "10120887492997993635"
     }
    }
   },
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "@torch.no_grad()\n",
    "def test(i: int, name: str, data_set: torch.utils.data.Dataset, data_loader: torch.utils.data.DataLoader):\n",
    "    model.eval()\n",
    "    cnt = 0\n",
    "    tot = len(data_set)\n",
    "    ls = 0\n",
    "    for data, lbl in data_loader:\n",
    "        data = data.to(device)\n",
    "        lbl = lbl.to(device)\n",
    "        oup = model(data)\n",
    "        out = torch.argmax(oup, dim=1)\n",
    "        targ = torch.argmax(lbl, dim=1)\n",
    "        cnt += torch.sum(out == targ).item()\n",
    "        ls += loss(oup, lbl).item()\n",
    "    ls /= len(data_loader)\n",
    "\n",
    "    print()\n",
    "    print(name)\n",
    "    print(f'Correct: {cnt}')\n",
    "    print(f'Wrong: {tot - cnt}')\n",
    "    print(f'Total: {tot}')\n",
    "    print(f'Correctness: {round(cnt / tot, 5)}')\n",
    "    print(f'Loss: {round(ls, 5)}')\n",
    "    print(\"=\" * 40)\n",
    "    model.train()\n"
   ],
   "metadata": {
    "id": "C7aZcZ3SyKy4",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1676578544488,
     "user_tz": 300,
     "elapsed": 3,
     "user": {
      "displayName": "datas xktz",
      "userId": "10120887492997993635"
     }
    }
   },
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1:\n",
      "Loss: 0.62705\n",
      "\n",
      "Test:\n",
      "Correct: 18032\n",
      "Wrong: 6968\n",
      "Total: 25000\n",
      "Correctness: 0.72128\n",
      "Loss: 0.55222\n",
      "========================================\n",
      "Epoch 2:\n",
      "Loss: 0.5362\n",
      "\n",
      "Test:\n",
      "Correct: 18347\n",
      "Wrong: 6653\n",
      "Total: 25000\n",
      "Correctness: 0.73388\n",
      "Loss: 0.53028\n",
      "========================================\n",
      "Epoch 3:\n",
      "Loss: 0.51705\n",
      "\n",
      "Test:\n",
      "Correct: 18360\n",
      "Wrong: 6640\n",
      "Total: 25000\n",
      "Correctness: 0.7344\n",
      "Loss: 0.53073\n",
      "========================================\n",
      "Epoch 4:\n",
      "Loss: 0.50232\n",
      "\n",
      "Test:\n",
      "Correct: 19006\n",
      "Wrong: 5994\n",
      "Total: 25000\n",
      "Correctness: 0.76024\n",
      "Loss: 0.49397\n",
      "========================================\n",
      "Epoch 5:\n",
      "Loss: 0.47926\n",
      "\n",
      "Train:\n",
      "Correct: 19667\n",
      "Wrong: 5333\n",
      "Total: 25000\n",
      "Correctness: 0.78668\n",
      "Loss: 0.45908\n",
      "========================================\n",
      "\n",
      "Test:\n",
      "Correct: 19400\n",
      "Wrong: 5600\n",
      "Total: 25000\n",
      "Correctness: 0.776\n",
      "Loss: 0.47291\n",
      "========================================\n",
      "Epoch 6:\n",
      "Loss: 0.45145\n",
      "\n",
      "Test:\n",
      "Correct: 19769\n",
      "Wrong: 5231\n",
      "Total: 25000\n",
      "Correctness: 0.79076\n",
      "Loss: 0.44554\n",
      "========================================\n",
      "Epoch 7:\n",
      "Loss: 0.43772\n",
      "\n",
      "Test:\n",
      "Correct: 19889\n",
      "Wrong: 5111\n",
      "Total: 25000\n",
      "Correctness: 0.79556\n",
      "Loss: 0.44353\n",
      "========================================\n",
      "Epoch 8:\n",
      "Loss: 0.4233\n",
      "\n",
      "Test:\n",
      "Correct: 19848\n",
      "Wrong: 5152\n",
      "Total: 25000\n",
      "Correctness: 0.79392\n",
      "Loss: 0.44614\n",
      "========================================\n",
      "Epoch 9:\n",
      "Loss: 0.41569\n",
      "\n",
      "Test:\n",
      "Correct: 20187\n",
      "Wrong: 4813\n",
      "Total: 25000\n",
      "Correctness: 0.80748\n",
      "Loss: 0.41912\n",
      "========================================\n",
      "Epoch 10:\n",
      "Loss: 0.40715\n",
      "\n",
      "Train:\n",
      "Correct: 20494\n",
      "Wrong: 4506\n",
      "Total: 25000\n",
      "Correctness: 0.81976\n",
      "Loss: 0.39829\n",
      "========================================\n",
      "\n",
      "Test:\n",
      "Correct: 20330\n",
      "Wrong: 4670\n",
      "Total: 25000\n",
      "Correctness: 0.8132\n",
      "Loss: 0.41267\n",
      "========================================\n",
      "Epoch 11:\n",
      "Loss: 0.39674\n",
      "\n",
      "Test:\n",
      "Correct: 20544\n",
      "Wrong: 4456\n",
      "Total: 25000\n",
      "Correctness: 0.82176\n",
      "Loss: 0.396\n",
      "========================================\n",
      "Epoch 12:\n",
      "Loss: 0.39127\n",
      "\n",
      "Test:\n",
      "Correct: 20641\n",
      "Wrong: 4359\n",
      "Total: 25000\n",
      "Correctness: 0.82564\n",
      "Loss: 0.38969\n",
      "========================================\n",
      "Epoch 13:\n",
      "Loss: 0.38077\n",
      "\n",
      "Test:\n",
      "Correct: 20733\n",
      "Wrong: 4267\n",
      "Total: 25000\n",
      "Correctness: 0.82932\n",
      "Loss: 0.39001\n",
      "========================================\n",
      "Epoch 14:\n",
      "Loss: 0.37557\n",
      "\n",
      "Test:\n",
      "Correct: 20808\n",
      "Wrong: 4192\n",
      "Total: 25000\n",
      "Correctness: 0.83232\n",
      "Loss: 0.37745\n",
      "========================================\n",
      "Epoch 15:\n",
      "Loss: 0.37183\n",
      "\n",
      "Train:\n",
      "Correct: 20491\n",
      "Wrong: 4509\n",
      "Total: 25000\n",
      "Correctness: 0.81964\n",
      "Loss: 0.40113\n",
      "========================================\n",
      "\n",
      "Test:\n",
      "Correct: 20366\n",
      "Wrong: 4634\n",
      "Total: 25000\n",
      "Correctness: 0.81464\n",
      "Loss: 0.41139\n",
      "========================================\n",
      "Epoch 16:\n",
      "Loss: 0.37052\n",
      "\n",
      "Test:\n",
      "Correct: 20631\n",
      "Wrong: 4369\n",
      "Total: 25000\n",
      "Correctness: 0.82524\n",
      "Loss: 0.39302\n",
      "========================================\n",
      "Epoch 17:\n",
      "Loss: 0.36514\n",
      "\n",
      "Test:\n",
      "Correct: 20843\n",
      "Wrong: 4157\n",
      "Total: 25000\n",
      "Correctness: 0.83372\n",
      "Loss: 0.38171\n",
      "========================================\n",
      "Epoch 18:\n",
      "Loss: 0.35916\n",
      "\n",
      "Test:\n",
      "Correct: 20941\n",
      "Wrong: 4059\n",
      "Total: 25000\n",
      "Correctness: 0.83764\n",
      "Loss: 0.36852\n",
      "========================================\n",
      "Epoch 19:\n",
      "Loss: 0.35926\n",
      "\n",
      "Test:\n",
      "Correct: 20105\n",
      "Wrong: 4895\n",
      "Total: 25000\n",
      "Correctness: 0.8042\n",
      "Loss: 0.44357\n",
      "========================================\n",
      "Epoch 20:\n",
      "Loss: 0.35587\n",
      "\n",
      "Train:\n",
      "Correct: 21217\n",
      "Wrong: 3783\n",
      "Total: 25000\n",
      "Correctness: 0.84868\n",
      "Loss: 0.3462\n",
      "========================================\n",
      "\n",
      "Test:\n",
      "Correct: 21027\n",
      "Wrong: 3973\n",
      "Total: 25000\n",
      "Correctness: 0.84108\n",
      "Loss: 0.3625\n",
      "========================================\n",
      "Epoch 21:\n",
      "Loss: 0.35582\n",
      "\n",
      "Test:\n",
      "Correct: 20888\n",
      "Wrong: 4112\n",
      "Total: 25000\n",
      "Correctness: 0.83552\n",
      "Loss: 0.37514\n",
      "========================================\n",
      "Epoch 22:\n",
      "Loss: 0.34887\n",
      "\n",
      "Test:\n",
      "Correct: 20812\n",
      "Wrong: 4188\n",
      "Total: 25000\n",
      "Correctness: 0.83248\n",
      "Loss: 0.38119\n",
      "========================================\n",
      "Epoch 23:\n",
      "Loss: 0.34902\n",
      "\n",
      "Test:\n",
      "Correct: 20598\n",
      "Wrong: 4402\n",
      "Total: 25000\n",
      "Correctness: 0.82392\n",
      "Loss: 0.40382\n",
      "========================================\n",
      "Epoch 24:\n",
      "Loss: 0.34448\n",
      "\n",
      "Test:\n",
      "Correct: 21125\n",
      "Wrong: 3875\n",
      "Total: 25000\n",
      "Correctness: 0.845\n",
      "Loss: 0.36177\n",
      "========================================\n",
      "Epoch 25:\n",
      "Loss: 0.34393\n",
      "\n",
      "Train:\n",
      "Correct: 21258\n",
      "Wrong: 3742\n",
      "Total: 25000\n",
      "Correctness: 0.85032\n",
      "Loss: 0.34389\n",
      "========================================\n",
      "\n",
      "Test:\n",
      "Correct: 21039\n",
      "Wrong: 3961\n",
      "Total: 25000\n",
      "Correctness: 0.84156\n",
      "Loss: 0.36169\n",
      "========================================\n",
      "Epoch 26:\n",
      "Loss: 0.34311\n",
      "\n",
      "Test:\n",
      "Correct: 20707\n",
      "Wrong: 4293\n",
      "Total: 25000\n",
      "Correctness: 0.82828\n",
      "Loss: 0.38837\n",
      "========================================\n",
      "Epoch 27:\n",
      "Loss: 0.33892\n",
      "\n",
      "Test:\n",
      "Correct: 21221\n",
      "Wrong: 3779\n",
      "Total: 25000\n",
      "Correctness: 0.84884\n",
      "Loss: 0.35403\n",
      "========================================\n",
      "Epoch 28:\n",
      "Loss: 0.34068\n",
      "\n",
      "Test:\n",
      "Correct: 21212\n",
      "Wrong: 3788\n",
      "Total: 25000\n",
      "Correctness: 0.84848\n",
      "Loss: 0.35178\n",
      "========================================\n",
      "Epoch 29:\n",
      "Loss: 0.33493\n",
      "\n",
      "Test:\n",
      "Correct: 20938\n",
      "Wrong: 4062\n",
      "Total: 25000\n",
      "Correctness: 0.83752\n",
      "Loss: 0.37606\n",
      "========================================\n",
      "Epoch 30:\n",
      "Loss: 0.33362\n",
      "\n",
      "Train:\n",
      "Correct: 21302\n",
      "Wrong: 3698\n",
      "Total: 25000\n",
      "Correctness: 0.85208\n",
      "Loss: 0.34469\n",
      "========================================\n",
      "\n",
      "Test:\n",
      "Correct: 20992\n",
      "Wrong: 4008\n",
      "Total: 25000\n",
      "Correctness: 0.83968\n",
      "Loss: 0.36818\n",
      "========================================\n",
      "Epoch 31:\n",
      "Loss: 0.33307\n",
      "\n",
      "Test:\n",
      "Correct: 21252\n",
      "Wrong: 3748\n",
      "Total: 25000\n",
      "Correctness: 0.85008\n",
      "Loss: 0.34403\n",
      "========================================\n",
      "Epoch 32:\n",
      "Loss: 0.33454\n",
      "\n",
      "Test:\n",
      "Correct: 20900\n",
      "Wrong: 4100\n",
      "Total: 25000\n",
      "Correctness: 0.836\n",
      "Loss: 0.37438\n",
      "========================================\n",
      "Epoch 33:\n",
      "Loss: 0.32952\n",
      "\n",
      "Test:\n",
      "Correct: 20988\n",
      "Wrong: 4012\n",
      "Total: 25000\n",
      "Correctness: 0.83952\n",
      "Loss: 0.36058\n",
      "========================================\n",
      "Epoch 34:\n",
      "Loss: 0.32613\n",
      "\n",
      "Test:\n",
      "Correct: 21208\n",
      "Wrong: 3792\n",
      "Total: 25000\n",
      "Correctness: 0.84832\n",
      "Loss: 0.3501\n",
      "========================================\n",
      "Epoch 35:\n",
      "Loss: 0.32582\n",
      "\n",
      "Train:\n",
      "Correct: 21292\n",
      "Wrong: 3708\n",
      "Total: 25000\n",
      "Correctness: 0.85168\n",
      "Loss: 0.34981\n",
      "========================================\n",
      "\n",
      "Test:\n",
      "Correct: 20956\n",
      "Wrong: 4044\n",
      "Total: 25000\n",
      "Correctness: 0.83824\n",
      "Loss: 0.3744\n",
      "========================================\n",
      "Epoch 36:\n",
      "Loss: 0.32402\n",
      "\n",
      "Test:\n",
      "Correct: 21022\n",
      "Wrong: 3978\n",
      "Total: 25000\n",
      "Correctness: 0.84088\n",
      "Loss: 0.36375\n",
      "========================================\n",
      "Epoch 37:\n",
      "Loss: 0.32448\n",
      "\n",
      "Test:\n",
      "Correct: 21348\n",
      "Wrong: 3652\n",
      "Total: 25000\n",
      "Correctness: 0.85392\n",
      "Loss: 0.33531\n",
      "========================================\n",
      "Epoch 38:\n",
      "Loss: 0.32387\n",
      "\n",
      "Test:\n",
      "Correct: 21238\n",
      "Wrong: 3762\n",
      "Total: 25000\n",
      "Correctness: 0.84952\n",
      "Loss: 0.34803\n",
      "========================================\n",
      "Epoch 39:\n",
      "Loss: 0.32016\n",
      "\n",
      "Test:\n",
      "Correct: 21219\n",
      "Wrong: 3781\n",
      "Total: 25000\n",
      "Correctness: 0.84876\n",
      "Loss: 0.34745\n",
      "========================================\n",
      "Epoch 40:\n",
      "Loss: 0.32045\n",
      "\n",
      "Train:\n",
      "Correct: 21668\n",
      "Wrong: 3332\n",
      "Total: 25000\n",
      "Correctness: 0.86672\n",
      "Loss: 0.31428\n",
      "========================================\n",
      "\n",
      "Test:\n",
      "Correct: 21314\n",
      "Wrong: 3686\n",
      "Total: 25000\n",
      "Correctness: 0.85256\n",
      "Loss: 0.34652\n",
      "========================================\n",
      "Epoch 41:\n",
      "Loss: 0.31912\n",
      "\n",
      "Test:\n",
      "Correct: 20607\n",
      "Wrong: 4393\n",
      "Total: 25000\n",
      "Correctness: 0.82428\n",
      "Loss: 0.40587\n",
      "========================================\n",
      "Epoch 42:\n",
      "Loss: 0.31558\n",
      "\n",
      "Test:\n",
      "Correct: 21270\n",
      "Wrong: 3730\n",
      "Total: 25000\n",
      "Correctness: 0.8508\n",
      "Loss: 0.34578\n",
      "========================================\n",
      "Epoch 43:\n",
      "Loss: 0.31524\n",
      "\n",
      "Test:\n",
      "Correct: 21256\n",
      "Wrong: 3744\n",
      "Total: 25000\n",
      "Correctness: 0.85024\n",
      "Loss: 0.34905\n",
      "========================================\n",
      "Epoch 44:\n",
      "Loss: 0.31416\n",
      "\n",
      "Test:\n",
      "Correct: 21063\n",
      "Wrong: 3937\n",
      "Total: 25000\n",
      "Correctness: 0.84252\n",
      "Loss: 0.35541\n",
      "========================================\n",
      "Epoch 45:\n",
      " 46 / 196 : Loss: 0.28799                       "
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-19-c2d1e79e7e82>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     13\u001B[0m         \u001B[0mls\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     14\u001B[0m         \u001B[0moptim\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 15\u001B[0;31m         \u001B[0mls_avg\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0mls\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mitem\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     16\u001B[0m         \u001B[0mcnt\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     17\u001B[0m         \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf\"\\r {cnt} / {tot} : Loss: {round(ls.item(), 5)}                       \"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mend\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "EPOCH = 400\n",
    "for epoch in range(1, EPOCH + 1):\n",
    "    print(f\"Epoch {epoch}:\")\n",
    "    ls_avg = 0\n",
    "    cnt = 0\n",
    "    tot = len(train_data_loader)\n",
    "    for data, lbl in train_data_loader:\n",
    "        optim.zero_grad()\n",
    "        data = data.to(device)\n",
    "        lbl = lbl.to(device)\n",
    "        y = model(data)\n",
    "        ls = loss(y, lbl)\n",
    "        ls.backward()\n",
    "        optim.step()\n",
    "        ls_avg += ls.item()\n",
    "        cnt += 1\n",
    "        print(f\"\\r {cnt} / {tot} : Loss: {round(ls.item(), 5)}                       \", end=\"\")\n",
    "    print(f\"\\rLoss: {round(ls_avg / len(train_data_loader), 5)}\")\n",
    "    torch.save(model.state_dict(), f\"model/model_{epoch}\")\n",
    "    test(epoch, \"Test:\", imdb_test_set, test_data_loader)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "FnADYxgxBkPU",
    "outputId": "491fee96-d4cd-48ce-f091-4b26aab840ac",
    "pycharm": {
     "is_executing": true
    },
    "executionInfo": {
     "status": "error",
     "timestamp": 1676586812659,
     "user_tz": 300,
     "elapsed": 1735677,
     "user": {
      "displayName": "datas xktz",
      "userId": "10120887492997993635"
     }
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": []
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
